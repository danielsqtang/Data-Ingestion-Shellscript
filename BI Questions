#Setup for each shell script, one for each question.

#!/bin/bash
#Initialize pyspark2
pyspark2 <<EOF
#Setup SparkContext and SparkConf
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("NorthwindDenormalizer")
sc = SparkContext(conf=conf)
#Setup HiveContext for importing Hive tables
from pyspark.sql import HiveContext
hive_context = HiveContext(sc)
#Import tables
Orders = hive_context.table("NorthwindOrders")
OrderDetails = hive_context.table("NorthwindOrderDetails")
Customers = hive_context.table("NorthwindCustomers")
Products = hive_context.table("NorthwindProducts")
Flattened = hive_context.table("NorthwindFlattened")
#Statements
...
...
EOF


  1.Which country has the most customers? Which city has the most customers?
// = Customers.filter(Customers['country'] != null)
  2.Which country has the most orders? Which city has the most orders?
//
  3.What are the top ten products in revenue for 1997?
Flattened1997 = Flattened.filter(pyspark.sql.functions.col("OrderDate").between('1997-01-01 00:00:00', '1997-12-30 00:00:00'))
FlattenedWithRev = Flattened.withColumn("TotalRev", Flattened.quantity*Flattened.unitprice)
totalrevenue = FlattenedWithRev.groupBy().sum('TotalRev')
totalrevenue.collect()
$968714
  4.Which product is the most expensive? How much revenue has this product generated in the 1997 year?
mostExpensive = Flattened.agg({"unitprice": "max"})
mostExpensive.collect()
$263
Flattened1997WithRev = FlattenedWithRev.filter(pyspark.sql.functions.col("OrderDate").between('1997-01-01 00:00:00', '1997-12-30 00:00:00'))
mostExpensive1997 = Flattened1997WithRev.filter("unitprice=263")
mostExpensive1997Rev = mostExpensive1997.groupBy().sum('TotalRev')
mostExpensive1997Rev.collect()
$22092
  5.How many products are available to purchase at this time? Which products are the least purchased?
  df.where(col("dt_mvmt").isNull())
productsAvailable = Products.filter("discontinued=0")
productsAvailable.count()
69 products available
  6.How many products are no longer available to puchase at this time?
productsNotAvailable = Products.filter("discontinued=1")
productsNotAvailable.count()
8 products unavailable
  7.What shipping company provides the most shipping service to us in 1997?
orders1997 = Orders.filter(pyspark.sql.functions.col("OrderDate").between('1997-01-01 00:00:00', '1997-12-30 00:00:00'))
shippingCounts = orders1997.groupBy('shipvia').count()
shippingCounts.show()
1-Speedy Express-131
3-United Package-121
2-Federal Shipping-151 has done the most shipping
